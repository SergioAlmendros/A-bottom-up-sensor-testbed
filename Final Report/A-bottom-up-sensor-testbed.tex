\documentclass[12pt, a4paper,twoside]{tesi_upf}
\renewcommand{\baselinestretch}{1.2}
\addtolength{\oddsidemargin}{+.550in}
\addtolength{\evensidemargin}{-.550in}
\usepackage{pgfgantt}
\usepackage{tikz}
\usepackage{eurosym}

\usetikzlibrary{shapes.geometric, arrows}

\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=red!30]

\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!30]

\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black, fill=orange!30]

\tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=green!30]

\tikzstyle{arrow} = [thick,->,>=stealth]

%CODIFICACIÓ
\usepackage[latin1]{inputenc}


%IDIOMES
\usepackage[catalan,english]{babel}

%NOMÉS PER A OBTENIR INDICACIÓ DEL MARC EN MIDA A4
\usepackage[cam,a4,center,frame]{crop}

%PER A INCLOURE GRÀFICS I EL LOGO DE LA UPF
\usepackage{graphicx}
\usepackage{caption}
\usepackage{acronym}
\usepackage{multirow}
%FONTS TIMES O GARAMOND, 
\usepackage{times}
%\usepackage{garamond}
\usepackage{url}

\usepackage{pdfpages}
%SENSE HEADINGS: NO MODIFICAR
\pagestyle{plain}

%PER A L'ÍNDEX DE MATÈRIES
\usepackage{makeidx}
\makeindex

%ESTIL DE BIBLIOGRAFIA
\bibliographystyle{apalike}


%AQUEST DOCUMENT ÉS EN CATALÀ
\selectlanguage{english}

%EN COMPTES DE ÍNDEX, LA TAULA DE CONTINGUTS ES TITULA SUMARI
\addto\captionscatalan
  {\renewcommand{\contentsname}{\Large \sffamily Sumari}}

% ~~~~~~~~~~~~~~~~~~~~~~~~~~
% CUSTOM PACKAGES
% ~~~~~~~~~~~~~~~~~~~~~~~~~~
%\usepackage{hyperref}
\usepackage[hidelinks]{hyperref}
\usepackage{titlesec}
\setcounter{secnumdepth}{4}
\usepackage{pdfpages}

%AFEGIU EN AQUESTA PART LES VOSTRES DADES
\title{A bottom up sensor testbed}
\author{Sergio Almendros Díaz}
\thyear{2013}
\department{Departament de Tecnologies de la Informació i les Comunicacions (DTIC)}
\supervisor{Jaume Barceló}


\begin{document}

\pdfstringdefDisableCommands{%
\let\MakeUppercase\relax
}

\frontmatter

\maketitle

\cleardoublepage


%%%%%% Dedicatòria; si no es vol posar, comenteu fins a final de dedicatòria

\noindent Dedicatòria

\cleardoublepage

%%%%%% Final de dedicatòria


%%%%%% Agraïments; si no es vol posar, comenteu fins a final de agraïments
\noindent {\Large \sffamily Acknowledgments}
\\[12pt] 
 
Acknowledgments

\cleardoublepage

%%%%%% Final dels agraïments

%ABSTRACT EN DOS IDIOMES. COM A MÍNIM CATALÀ. SI L'ALTRE ÉS EN CASTELLA CANVIEU EL QUE POSA ABSTRACT
\selectlanguage{english}
\section*{\Large \sffamily Abstract}

Bottom up defines the network design, deployment, and operation initiatives driven by end user needs\footnote{\url{http://bubforeurope.net}}. Sensor testbed is a deployment of several sensors over a defined location.

In this project we will develop a bottom up sensor testbed, which means a deployment of a sensor testbed driven by end user needs. The sensor nodes will consist of an Arduino\footnote{\url{http://arduino.cc/}} YUN (Arduino is an open-source electronics prototyping platform) and 5 environmental sensors attach to it. 

The arduino YUN is connected to Guifi\footnote{\url{http://guifi.net/}}, an open network built to everyone can join it providing his own connection. The Guifi nodes are Power Over Ethernet (PoE), and the Arduino YUN can be connected to it with a PoE module, but there is no module yet.

The arduino send the sensory data to opencities\footnote{\url{http://opendata.nets.upf.edu/web/index.php/en/}}, a platform to browse, visualize, and download open data from different participants.

Finally, an Android aplication shows the data from opencities.

At the end of this thesis we made a real testbed with 3 nodes, it shows that the data is send correctly and without any interruption.


\selectlanguage{catalan}
\vspace*{\fill}
\section*{\Large \sffamily  Resum}

Cuando el Abstract esté perfecto, lo traduciré al catalan.

\vspace*{\fill}

\selectlanguage{english}
\cleardoublepage
%FIN DE ABSTRACTE

%PREFACI OPCIONAL. SI NO ES VOL, COMENTEU FINS EL FINAL DE PREFACI
%{\bf Prefaci}
%
%\cleardoublepage
%FINAL DE PREFACI


%TAULA DE CONTINGUTS: OBLIGATÒRIA
\selectlanguage{english}
\tableofcontents

%INDEX DE FIGURES; NOMÉS ES POSA SI HI HA FIGURES
\listoffigures
%Fa que aparegui al sumari
\addcontentsline{toc}{chapter}{List of figures}

%INDEX DE TAULES; NOMÉS ES POSA SI HI HA TAULES
\listoftables
%Fa que aparegui al sumari
\addcontentsline{toc}{chapter}{List of tables}


%COMENÇA EL TEXT
\mainmatter
\chapter{Introduction}
\label{Chapter1}

  The development of this project involves two parts, recollect sensory data and show it.

  A sensor testbed is a sensor network that has the goal to gather sensory data, and test the technologies used as nodes to see if they are the best option.

  Bottom-up is, basically, the pattern that we used to build the sensor testbed, where the end users, in this case, guifi.net users, are the ones who have to assemble the sensor nodes and attached them to their guifi nodes to create the sensor network. With the bottom-up model, the data is provide and use by the end users.
  This project is an easy way to understand the importance of sensor networks and how they can help us to know, for example, if there is low quality air in our city, and do something about it.

  As sensor nodes we will use an Arduino YUN, which allows the user to obtain analog reads from a sensor very easily and, with a Power over Ethernet module, it can be attached to guifi nodes and send the sensory data to a sensor platform, like opencities.

  When the sensory data is stored, we will develop an Android application to visualize this data and make it more accessible to other users not involved with guifi.net.

  In the following chapters I will explain the state of sensor networks nowadays (Chapter \ref{Chapter2}), which technologies we will use (Chapter \ref{Chapter3}), and how the project has been done (Chapter \ref{Chapter4}). 

  Finally there will be the results (Chapter \ref{Chapter5}) of the testbed, and conclusions (Chapter \ref{Chapter6}) and future work (Chapter \ref{Chapter7}).
  
\chapter{State of the Art}
\label{Chapter2}
  
  \section{Introduction}
    Sensor networks started as a mecanism of defense developed by the military during the Cold War, with acoustic sensors they tried to find Soviet submarines. This search continued at universities, trying to make this sensors smaller, and with the posibility of real-time data\cite{chong2003sensor}.
    
    Right now, sensors are small enough, and processors with network technology have low energy consumption, which allows us to deploy a test bed without people notice it.
    
    Smart cities are the next step, a city capable of having real-time information, not only about the environment, it can go from the amount of cars that pass a road, to the amount of rain water in a day. This kind of information could help to manage more efficiently the city.
    
    It is important to share this information, in the case that the government build the sensor network, the data should be open to everyone could see it. There are already some sensor networks functioning, some of them are from the government, and, sometimes, there are not that open about their data, but There are also some people who have sensors nodes at home and share the information with everyone.

  \section{Sensor networks and smart cities}
    In this section we introduce a few projects of sensor networks deployed:
    
    \subsection{Amsterdam smart city}
      Amsterdam have a lot of projects concerning the smart city concept, like the "Flexible street lighting", which allows the government to monitor the street and switch off the lights to save energy, or the "Smart parking" which let drivers to know if there are free spots to park, and, in consecuence, reduce air pollution\cite{SmartcityAmsterdam}.
      
    \subsection{Santander smart city}
      Santander has his own sensor network testbed for environmental monitoring, outdoor parking area management, or traffic intensity monitoring\cite{SmartcitySantander}.
    
    
  \section{Companies}
    There are some companies that are in the business of sensor networks, such as "Schneider Electric", a multinational company that produces components for energy management, or smartcitizen, a platform that allows a user to have a sensor node and share the data with everyone.

    
  \section{Opendata services}
    The sensor networks are useless if we don't store the data, although we could save it in the device, it would be too expensive to recollect it. That is why we have chosen Opencities\footnote{\url{http://opendata.nets.upf.edu/web/index.php/en/}}, a platform to browse, visualize, and download open data from different participants. We will only use this platform to store and download the data in the Android App.
    
    There are some similar services such as Xively, or sentilo, but opencities is develop in the Pompeu Fabra University, what means that if I have any problem, a solution will be found more quickly.

\chapter{Technologies}
\label{Chapter3}
  
  In this chapter we focused in the techonologies used to develop this project, which are divided in four parts: arduino, sensors, python, Guifi network and opencities, and Android. 

  
  \section{Arduino}
    The Arduino board that we use is an arduino YUN\footnote{\url{http://arduino.cc/en/Main/ArduinoBoardYun?from=Main.ArduinoYUN}} as we can see in the figure \ref{fig:AYUN}, it supports a Linux distribution based on OpenWRT named Linino, it has Ethernet and wifi suport, and a micro-SD card slot, those are basically the reason why we decided to use it, as we have to store the recollected data, and send it to opencities through the Internet.
    
    We have also planned to attached a power over ethernet (PoE) module to the arduino to make more clean the deployment.
    
    \begin{figure}[htbp]
        \centering
            \includegraphics[scale=0.5]{./Figures/ArduinoYunFront_2_450px.jpg}
            \rule{18em}{0.5pt}
        \caption[Arduino YUN]{Arduino YUN.}
        \label{fig:AYUN}
    \end{figure}

  \section{Sensors}
    A sensor is a device which transform a physical measure to an output signal that can be read by another device, such as an arduino.
    
    In this project we will use five sensors that measured temperature, light, noise, humidity, and gas.
    
    To show how the sensors are connected to the arduino YUN I used the program fritzing\footnote{\url{http://fritzing.org/}}.
    
    \subsection{LM35: Temperature}
      LM35 [Figure \ref{fig:LM35}] is a sensor to mesure temperature, in the figure \ref{fig:TemperatureSensor_bb} we can see the way to connect it to the arduino.

      \begin{figure}[htbp]
          \centering
              \includegraphics[scale=0.5]{./Figures/LM35.jpg}
              \rule{25em}{0.5pt}
          \caption[LM35 sensor]{LM35 temperature sensor.}
          \label{fig:LM35}
      \end{figure}
      
      \begin{figure}[htbp]
          \centering
              \includegraphics[scale=1]{./Figures/Fritzing/TemperatureSensor_bb.png}
              \rule{25em}{0.5pt}
          \caption[Temperature Sensor Breadboard]{Temperature Sensor Breadboard.}
          \label{fig:TemperatureSensor_bb}
      \end{figure}
      
    \subsection{Light Dependent Resistor (LDR)}
      LDR [Figure \ref{fig:LDR}] is a light sensor, in the figure \ref{fig:LightSensor_bb} we can see the way to connect it to the arduino.

      \begin{figure}[htbp]
          \centering
              \includegraphics[scale=0.3]{./Figures/LDR.jpg}
              \rule{25em}{0.5pt}
          \caption[LDR sensor]{photoresistor or light-dependent resistor.}
          \label{fig:LDR}
      \end{figure}
      
      \begin{figure}[htbp]
          \centering
              \includegraphics[scale=1]{./Figures/Fritzing/LightSensor_bb.png}
              \rule{25em}{0.5pt}
          \caption[Light Sensor Breadboard]{Light Sensor Breadboard.}
          \label{fig:LightSensor_bb}
      \end{figure}
 
    \subsection{Emartee Mini Sound Sensor and Analog Sound Sensor Board Microphone MIC Controller: Noise}
      This two sensors are used to measured noise levels [Figure \ref{fig:MSS}] and [Figure \ref{fig:Analognoisesensor}]. The code to read the noise values is the same for both. In the figure \ref{fig:NoiseSensor_bb} we can see the way to connect them to the arduino.

      \begin{figure}[!htbp]
          \centering
              \includegraphics[scale=0.5]{./Figures/MiniSoundSensor.jpg}
              \rule{25em}{0.5pt}
          \caption[Mini Sound Sensor]{Mini Sound Sensor.}
          \label{fig:MSS}
      \end{figure}
      
      \begin{figure}[!htbp]
          \centering
              \includegraphics[scale=0.5]{./Figures/analognoisesensor.jpg}
              \rule{25em}{0.5pt}
          \caption[Analog noise sensor]{Analog noise sensor.}
          \label{fig:Analognoisesensor}
      \end{figure}

      
      \begin{figure}[!htbp]
          \centering
              \includegraphics[scale=1]{./Figures/Fritzing/NoiseSensor_bb.png}
              \rule{25em}{0.5pt}
          \caption[Noise Sensor Breadboard]{Noise Sensor Breadboard.}
          \label{fig:NoiseSensor_bb}
      \end{figure}


    \subsection{Aosong DHT22 and DHT11: Humidity}
      DHT22 [Figure \ref{fig:DHT22}] and DHT11 [Figure \ref{fig:DHT11}] are humidity and temperature sensors, although we will only use the humidity measure. The output is digital, and to read it, we use an external library\footnote{\url{https://github.com/adafruit/DHT-sensor-library}}. The arduino and the humidity sensor will be connected as shown in the figure \ref{fig:HumiditySensordht22_bb}. 
      
      At the time I started the testbed, there was only 1 DHT22 sensor, so I had to use two DHT11 sensor, which change the breadboard a little bit as shown in the figure \ref{fig:HumiditySensordht11_bb}.

      \begin{figure}[htbp]
          \centering
              \includegraphics[scale=0.5]{./Figures/dht22.jpg}
              \rule{25em}{0.5pt}
          \caption[DHT22 sensor]{DHT22 humidity and temperature sensor.}
          \label{fig:DHT22}
      \end{figure}
      
      \begin{figure}[htbp]
          \centering
              \includegraphics[scale=0.5]{./Figures/dht11.png}
              \rule{25em}{0.5pt}
          \caption[DHT11 sensor]{DHT11 humidity and temperature sensor.}
          \label{fig:DHT11}
      \end{figure}
      
      \begin{figure}[!htbp]
          \centering
              \includegraphics[scale=1]{./Figures/Fritzing/HumiditySensor_bb.png}
              \rule{25em}{0.5pt}
          \caption[Humidity Sensor DHT22 Breadboard]{Humidity Sensor DHT22 Breadboard.}
          \label{fig:HumiditySensordht22_bb}
      \end{figure}
      
      \begin{figure}[!htbp]
          \centering
              \includegraphics[scale=1]{./Figures/Fritzing/HumiditySensorDht11_bb.png}
              \rule{25em}{0.5pt}
          \caption[Humidity Sensor DHT11 Breadboard]{Humidity Sensor DHT11 Breadboard.}
          \label{fig:HumiditySensordht11_bb}
      \end{figure}
      
      
    \subsection{MQ135: Gas sensor}
      This is a gas sensor [Figure \ref{fig:MQ135}], and we will use it to measure air quality. This sensor does not have a figure in fritzing, so I used a gas sensor that has the same output, and int the figure \ref{fig:GasSensor_bb} we can see how to connect it to the arduino.

      \begin{figure}[htbp]
          \centering
              \includegraphics[scale=0.3]{./Figures/mq135.jpg}
              \rule{25em}{0.5pt}
          \caption[MQ135]{MQ135 Air Quality sensor.}
          \label{fig:MQ135}
      \end{figure}
      
      \begin{figure}[!htbp]
          \centering
              \includegraphics[scale=1]{./Figures/Fritzing/GasSensor_bb.png}
              \rule{25em}{0.5pt}
          \caption[Gas Sensor Breadboard]{Gas Sensor Breadboard.}
          \label{fig:GasSensor_bb}
      \end{figure}
      
    \subsection{BreadBoard with all the sensors}
      
      In the figure \ref{fig:AllSensors22_bb} we can see the final prototype with the DHT22 sensor, and in the figure \ref{fig:AllSensors11_bb} with the DHT11 sensor.
      
      \begin{figure}[!htbp]
          \centering
              \includegraphics[scale=0.8]{./Figures/Fritzing/AllSensors22_bb.png}
              \rule{25em}{0.5pt}
          \caption[Sensor node Prototype with DHT22]{Sensor node Prototype with DHT22.}
          \label{fig:AllSensors22_bb}
      \end{figure}
      
      \begin{figure}[!htbp]
          \centering
              \includegraphics[scale=0.8]{./Figures/Fritzing/AllSensors11_bb.png}
              \rule{25em}{0.5pt}
          \caption[Sensor node Prototype with DHT11]{Sensor node Prototype with DHT11.}
          \label{fig:AllSensors11_bb}
      \end{figure}
    
  \section{Python}
    Because of the low memory for the arduino sketches, we have to use a python script to communicate with opencities. The version of python in the arduino is the 4.2.5.
    
  \section{Guifi network and opencities}
    Guifi \footnote{\url{https://www.guifi.net/}} is the network where the arduino's will be deployed, and the one providing the access to Opencities through the Internet.
    
    Opencities\footnote{\url{http://opencities.upf.edu/web/index.php/en/}} is the opendata services that we have chosen, the strengths of opencities are:
    \begin{itemize}
      \item They give us free storage.
      \item Easy API to upload and download the data.
      \item The developers are in the UPF and problems can be solved more easily.
    \end{itemize}
    
  \section{Android}
    Android is an open source mobile operating system from Google, it runs on smartphones, and we will use it to develop an application to see the sensory data stored in opencities, and show it to the user in a way that anybody can understand it.
    
    This application it will be tested on a Sony Xperia Z1, with an Android 4.4.2.

\chapter{Bottom up sensor testbed}
\label{Chapter4}

  This chapter focused in the process that has been followed to complete the project, which has two main parts, the software to recollect and send the data and the Android application to show it.
  
  \section{Collect and Send}
    
    We will need to scripts, one to collect the data, and other to send it. This is because the memory to run an arduino sketch is very low, and the creation of the message to opencities is too big. That is why we use a python script called by the arduino sketch.
    
    To get started with how the arduino YUN visit this website\footnote{\url{http://arduino.cc/en/Guide/ArduinoYun}}.
    
    The arduino sketch is responsible for collecting the data, write it down in a logData file, and call the python script with the collected values and an unique ID, and the python script have to create a GeoJSON and send it to opencities.
    
    \subsection{Collect sensory data}
    
      To collect almost all the data the skecth does not need to include any libray because it is read by the analog read, except for the humidity sensors (DHT22 and DHT11) which need an external library.
      
      To read and write into the logData file we need the FileIO library\footnote{\url{http://arduino.cc/en/Reference/YunFileIOConstructor}}, and to call the python script we need the Process library\footnote{\url{http://arduino.cc/en/Reference/YunProcessConstructor}}.
      
      The figure \ref{fig:Arduino sketch Flow Chart} explain how the arduino sketch works.\\
      
      \begin{figure}[htbp]
      \centering

      \begin{tikzpicture}[node distance=2cm]

        \node (bridge) [startstop] {Initialize Bridge};
        \node (Serial) [process, below of=bridge] {Initialize Serial};
        \node (FileSystem) [process, below of=Serial] {Initialize FileSystem};
        \node (ReadSensors) [process, below of=FileSystem] {Read Sensors};
        \node (ReadFile) [process, below of=ReadSensors] {Read \& Write File};
        \node (delay) [process, right of=ReadFile, xshift=2cm] {Delay of 1 minute};
        \node (ExecutePython) [process, below of=ReadFile] {Execute Python Script};
        
        \draw [arrow] (bridge) -- (Serial);
        \draw [arrow] (Serial) -- (FileSystem);
        \draw [arrow] (FileSystem) -- (ReadSensors);
        \draw [arrow] (ReadSensors) -- (ReadFile);
        \draw [arrow] (ReadFile) -- (ExecutePython);
        \draw [arrow] (ExecutePython) -| (delay);
        \draw [arrow] (delay) |- (ReadSensors);

      \end{tikzpicture}
      \rule{35em}{0.5pt}
      \caption[Arduino sketch Flow Chart]{Arduino sketch Flow Chart.}
      \label{fig:Arduino sketch Flow Chart}
      \end{figure}
    
    \subsection{Communication with opencities}
    
      The communication with opencities is done by a python script, to do this we need a set of libraries installed in the arduino.
      
      We need the sys and datetime library that are already installed, but the geopy, geojson, and httplib2 libraries has to be installed.
      
      This are the steps I followed to install the libraries:
      
      \begin{enumerate}
        \item First we need to configure the onboard wifi, in this website it is explained how\footnote{\url{http://arduino.cc/en/Guide/ArduinoYun}}
        \item When the YUN have an IP, now we can get into the linino by Secure Shell: 
          \\ {\centering\textbf{ssh root@X.Y.Z.W}\par}
        \item Now that we are in the linino, we begin to install the necesary packets:
          \\ {\centering
            \textbf{
              opkg update \\
              opkg install distribute \\
              opkg install python-openssl \\
              easy\_install pip \\
              pip install geojson \\
              pip install geopy \\
              pip install httplib2
            }
          \par}
      \end{enumerate}
      
      With all this libraries we can communicate with opencities and store the sensory data recollected by the arduino. The \ref{fig:Python Script Flow Chart} figure explain how the python script works.\\
      
      \begin{figure}[!htbp]
      \centering

      \begin{tikzpicture}[node distance=2cm]      
        
        \node (start) [startstop] {Called by the arduino};
        \node (CorrectUsage) [decision, below of=start, yshift=-2cm] {Is the usage correct};
        \node (collectdata) [process, below of=CorrectUsage, yshift=-2cm] {Store the arguments as sensory data};
        \node (ShowCorrectWay) [process, right of=CorrectUsage, xshift=4cm] {Show the correct usage};
        \node (StopUsage) [startstop, right of=ShowCorrectWay, xshift=2cm] {Stop};
        \node (CreateGeoJSON) [process, below of=collectdata] {Create GeoJSON};
        \node (PostOpencities) [process, below of=CreateGeoJSON] {POST in Opencities};
        \node (stop) [startstop, below of=PostOpencities] {Stop};
        
        \draw [arrow] (start) -- (CorrectUsage);
        \draw [arrow] (CorrectUsage) -- node[anchor=east] {yes} (collectdata);
        \draw [arrow] (CorrectUsage) -- node[anchor=south] {no} (ShowCorrectWay);
        \draw [arrow] (ShowCorrectWay) -- (StopUsage);
        \draw [arrow] (collectdata) -- (CreateGeoJSON);
        \draw [arrow] (CreateGeoJSON) -- (PostOpencities);
        \draw [arrow] (PostOpencities) -- (stop);
        

      \end{tikzpicture}
      \rule{35em}{0.5pt}
      \caption[Python Script Flow Chart]{Python Script Flow Chart.}
      \label{fig:Python Script Flow Chart}
      \end{figure}
      
  \section{Android app}
  
    \subsection{Summary}

      To make easy to see the results of the testbed, I developed an android application. The application shows the data of the sensors in two ways, with markers that show the actual environmental value in that point, and also with heatmap points, the larger the value, the more intense the red will be. This can be seen in figure \ref{fig:App_Screenshot_3}.
      
      \begin{figure}[htbp]
          \centering
              \includegraphics[scale=0.1]{./Figures/App_Screenshot_3.png}
              \rule{35em}{0.5pt}
          \caption[App Screenshot 3]{App Screenshot.}
          \label{fig:App_Screenshot_3}
      \end{figure}
      
    \subsection{Interface}
      The application interface is a unique map view where the user can zoom to a limit, go to their location, and use the top buttons. From left to right, the first button is the Marker button, the user decides whether the markers are displayed or not, and the next buttons refer to the type of sensor data to show as markers and/or as heatmap points (Temperature, Humidity, Noise, Light, and Air Quality).
      If the Marker button is checked, the user can click on the marker in the map and it will show the value of the temperature, humidity,... and the unit.
      In the figure \ref{fig:App_Screenshot_1} we can see the app with the Marker button checked, and in the figure \ref{fig:App_Screenshot_2} without.
      
      \begin{figure}[htbp]
          \centering
              \includegraphics[scale=0.1]{./Figures/App_Screenshot_1.png}
              \rule{35em}{0.5pt}
          \caption[App Screenshot 1]{App Screenshot.}
          \label{fig:App_Screenshot_1}
      \end{figure}
      
      \begin{figure}[htbp]
          \centering
              \includegraphics[scale=0.1]{./Figures/App_Screenshot_2.png}
              \rule{35em}{0.5pt}
          \caption[App Screenshot 2]{App Screenshot.}
          \label{fig:App_Screenshot_2}
      \end{figure}
      
    \subsection{Code}
      First of all, to create this application I have used the Google Maps Android API v2\footnote{\url{https://developers.google.com/maps/documentation/android/}} for the map view, and the Google Maps Android API utility library\footnote{\url{http://googlemaps.github.io/android-maps-utils/}} for the heatmaps. 
    
      This application has the next classes:
      \begin{itemize}
        \item MainActivity: Is the controller of the whole application.
        \item GPSTracker: Is a class to get the current location of the user.
        \item Feature, Geometry, and Properties: This are the classes where it will stored the data from the parsed JSON from opendata.
        \item DataBase: This is a singleton class where all the variables are stored, because is more easy to access from different clases.
      \end{itemize}
      
      In the figure \ref{fig:ClassDiagram1} and {fig:ClassDiagram2} are the class diagram:
      
      \begin{figure}[htbp]
        \centering
        \includegraphics[page=1,scale=0.65]{./AdditionalInformation/ClassDiagram1_1.pdf}
        \rule{35em}{0.5pt}
        \caption[Class Diagram of the Android App part 1]{Class Diagram of the Android App part 1.}
        \label{fig:ClassDiagram1}
      \end{figure}
      
      \begin{figure}[htbp]
        \centering
        \includegraphics[page=1,scale=0.65]{./AdditionalInformation/ClassDiagram1_2.pdf}
        \rule{35em}{0.5pt}
        \caption[Class Diagram of the Android App part 2]{Class Diagram of the Android App part 2.}
        \label{fig:ClassDiagram2}
      \end{figure}
      
      The explanation of the code is in the following flow chart \ref{fig:Android App Flow Chart}:
      
      \begin{figure}[!htbp]
      \centering

      \begin{tikzpicture}[node distance=1cm]      
        
        \node (start) [startstop] {Initialize Google Map};
        \node (zoomlimit) [process, below of=start, yshift=-0.5cm] {Set zoom limit};
        \node (InitializeButtons) [process, below of=zoomlimit, yshift=-0.5cm] {Initialize all the Buttons and listeners};
        \node (GetCurrentLocation) [process, below of=InitializeButtons, yshift=-1cm] {Get Current Location using GPSTracker};
        \node (CallOpendata) [process, below of=GetCurrentLocation, yshift=-1cm] {Call OpenData, and parse the JSON};
        \node (CreateFeaturesByCoordinates) [process, below of=CallOpendata, yshift=-2cm] {Create a hashmap with all the features with the  coordinates as key, and a list of features for every key};
        \node (addMarkers) [process, below of=CreateFeaturesByCoordinates, yshift=-2cm] {Save only the latest features for every coordinate};
        \node (addHeatMapAndaddMakers2) [process, below of=addMarkers, yshift=-1.5cm] {Show the markers or heatmap points of a type of data};
        
        \draw [arrow] (start) -- (zoomlimit);
        \draw [arrow] (zoomlimit) -- (InitializeButtons);
        \draw [arrow] (InitializeButtons) -- (GetCurrentLocation);
        \draw [arrow] (GetCurrentLocation) -- (CallOpendata);
        \draw [arrow] (CallOpendata) -- (CreateFeaturesByCoordinates);
        \draw [arrow] (CreateFeaturesByCoordinates) -- (addMarkers);
        \draw [arrow] (addMarkers) -- (addHeatMapAndaddMakers2);
        

      \end{tikzpicture}
      \rule{35em}{0.5pt}
      \caption[Android App Flow Chart]{Android App Flow Chart.}
      \label{fig:Android App Flow Chart}
      \end{figure}
  

\chapter{Testbed results}
\label{Chapter5}

  In this chapter I will explained the procedure I followed to make this testbed, and an explanation of the results.
  
  In the figure \ref{fig:TestBed_Prototype} there is a photograph of the prototype I will use in this testbed. It is composed of an Arduino YUN, a microSD card, a breadboard, and all the sensor connected (temperature, humidity, noise, light, and gas).
  
  \begin{figure}[htbp]
      \centering
          \includegraphics[scale=0.1]{./Figures/TestBed_Prototype.JPG}
          \rule{35em}{0.5pt}
      \caption[TestBed Prototype]{TestBed Prototype.}
      \label{fig:TestBed_Prototype}
  \end{figure} 
  
  \section{Sensor node}
  
  Now, I will show the process to configure the node, part of this process is taken of the website of arduino\footnote{\url{http://arduino.cc/en/Guide/ArduinoYun}}:
  
  \subsection{Connection to the Internet}
    First of all we have to provide of Internet connection to the arduino.
    \subsubsection{Through Ethernet}
      This is the fastest way to provide of Internet connection, the arduino will behave the same way as a computer, automatically will have an IP address.
      
    \subsubsection{Through WiFi}
      This is a slowest way. The process is the following:
      \begin{enumerate}
        \item First we power the arduino YUN. 
        \item The arduino will create his own WiFi network (ArduinoYun-XXXXXXXXXXXX), and with a computer we connect to it.
        \item When we are connected to the YUN network, we go to a web browser and go to \url{http://arduino.local} or \url{192.168.240.1}. We have to put the password, which is "arduino". As we can see in the figure \ref{fig:YunWebPassword}.
          \begin{figure}[htbp]
            \centering
                \includegraphics[scale=0.5]{./Figures/YunWebPassword.png}
                \rule{35em}{0.5pt}
            \caption[Yun web Password]{Yun web Password.}
            \label{fig:YunWebPassword}
          \end{figure}
        
        \item The next page will be an information page, click on the configure button \ref{fig:YunWebDiagnostic}.
        
          \begin{figure}[htbp]
            \centering
                \includegraphics[scale=0.5]{./Figures/YunWebDiagnostic.png}
                \rule{35em}{0.5pt}
            \caption[Yun web Diagnostic]{Yun web Diagnostic.}
            \label{fig:YunWebDiagnostic}
          \end{figure}
        
        \item We will give a unique name to the Yun, and the network we want to connect \ref{fig:YunWebConfig}.
          \begin{figure}[htbp]
            \centering
                \includegraphics[scale=0.5]{./Figures/YunWebDiagnostic.png}
                \rule{35em}{0.5pt}
            \caption[Yun web Configuration]{Yun web Configuration.}
            \label{fig:YunWebConfig}
          \end{figure}
          
        \item We press the configure \& restart button.
        \item Finally we have to connect to same network as the Yun is connected.
        
      \end{enumerate}
  \subsection{Install necessary packets}
    The arduino will run an arduino script and a python script, for the first one we do not have to install anything, but for python we will have to install some packets.
    
    To install any packet we first have to connect to the arduino (The password will be the one we entered when we configured the onboard WiFi or "arduino"):
    
    {\begin{centering} \textbf{ssh root@X.Y.Z.W}\par \end{centering}}
    
    Now we can install all the necessary packets:
     
      {\begin{centering} \textbf{
          opkg update \\
          opkg install distribute \\
          opkg install python-openssl \\
          easy\_install pip \\
          pip install geojson \\
          pip install geopy \\
          pip install httplib2
        }
      \par \end{centering}}
    
  \subsection{Copy the scripts}
    First of all we have to create the some directories, so once we make the ssh command, go to "/mnt/sda1", make the following comands:
    
    {\begin{centering} \textbf{
    mkdir arduino \\
    cd arduino \\
    mkdir www \\
    }\par \end{centering}}
    
    We have to copy the python script "main.py" into the SD-Card. We have two ways to do this, put the SDCard into a computer an save the files in there, or we can copy the files into the arduino through the network with the following comand:
    
    {\begin{centering} \textbf{
    scp main.py root@192.168.2.149:/mnt/sda1/arduino/www/main.py
    }\par \end{centering}}
  
  \subsection{Attach the sensors}
    Now that we have the python step done, we have to attach the sensors to the arduino Yun, to do that look at figure \ref{fig:AllSensors22_bb} or figure \ref{fig:AllSensors11_bb}.
  
  \subsection{Arduino Code}
    To upload an arduino sketch to the yun we have to use the following IDE: arduino 1.5.5. There are two ways to upload an sketch, through a USB cable connected to the arduino, or through the Internet, if we are in the same network as the arduino, it will appear in the IDE.
  
  \section{Actual Testbed}
    We have three arduino, so we have to mount the sensor node and put it in three different places in free space but being sure it will not get wet.
    
    When we have decided when we are going to put them, we have to introduce manually the location and the unique ID into the python script. We can do that by entering into the arduino by secure shell as we did earlier, go to the folder where the "main.py" is, and modified the following line by using "nano":
    
    {\begin{centering} \textbf{self.address = 'Sagrada Familia, Carrer de Mallorca, Barcelona'}\par \end{centering}}
    
    After a day of collecting data, we will get the arduino's back, take the three logData files, and show the data into some graphics.
    
    The figures \ref{} show the testbed I did.

\chapter{Conclusions}
\label{Chapter6}

\chapter{Future work}
\label{Chapter7}

\bibliography{bibliography}
\cleardoublepage

\chapter{Appendixes}
\label{Chapter8}

  \input{./Appendix/Pilot_Charter.tex}
  \input{./Appendix/Planning_Report.tex}


\backmatter
\printindex





\end{document}


%NUMERACIÓ DE LA PÀGINA EXTERIOR EXCEPTE EN LA PRIMERA PÀGINA DE CADA CAPÍTOL
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyfoot{}
\fancyfoot[RO]{\thepage}
\fancyfoot[LE]{\thepage}


%MUTIPLES ÍNDEX
%En el preàmbul
\usepackage{multind}
\makeindex{authors}
%Introducció d'entrades la forma
\index{authors}{Einstein}
%Situació de l'Índex
\printindex{authors}{Author index}
%Cal eliminar les comandes \usepakage{makeidx} \makeindex \printindex
%cal exacutar des de la línia de comandes makeindex authors
